{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvmQgzc9NqF0hTTyqgTfuE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-patankar/CPSC597/blob/main/SMOTE_and_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z35uNJe8iV_H",
        "outputId": "eb833319-a96c-4983-b898-f4c9048e1edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading creditcardfraud.zip to /content\n",
            " 99% 65.0M/66.0M [00:01<00:00, 72.9MB/s]\n",
            "100% 66.0M/66.0M [00:01<00:00, 65.8MB/s]\n",
            "Archive:  creditcardfraud.zip\n",
            "  inflating: creditcard.csv          \n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
        "!unzip creditcardfraud.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the credit card transaction dataset\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Balance the training dataset using SMOTE\n",
        "smote = SMOTE(random_state=2)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the supervised autoencoder architecture\n",
        "input_dim = X_train_resampled.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the supervised autoencoder\n",
        "model.fit(X_train_resampled, y_train_resampled,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cexHp1-i3wD",
        "outputId": "6c905117-af63-477c-ca24-8102f9d83471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3554/3554 [==============================] - 9s 2ms/step - loss: 0.0311 - val_loss: 0.0087\n",
            "Epoch 2/10\n",
            "3554/3554 [==============================] - 7s 2ms/step - loss: 0.0053 - val_loss: 0.0074\n",
            "Epoch 3/10\n",
            "3554/3554 [==============================] - 8s 2ms/step - loss: 0.0037 - val_loss: 0.0150\n",
            "Epoch 4/10\n",
            "3554/3554 [==============================] - 7s 2ms/step - loss: 0.0030 - val_loss: 0.0102\n",
            "Epoch 5/10\n",
            "3554/3554 [==============================] - 8s 2ms/step - loss: 0.0025 - val_loss: 0.0083\n",
            "Epoch 6/10\n",
            "3554/3554 [==============================] - 7s 2ms/step - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 7/10\n",
            "3554/3554 [==============================] - 9s 2ms/step - loss: 0.0019 - val_loss: 0.0088\n",
            "Epoch 8/10\n",
            "3554/3554 [==============================] - 8s 2ms/step - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 9/10\n",
            "3554/3554 [==============================] - 8s 2ms/step - loss: 0.0015 - val_loss: 0.0091\n",
            "Epoch 10/10\n",
            "3554/3554 [==============================] - 8s 2ms/step - loss: 0.0014 - val_loss: 0.0086\n",
            "1781/1781 [==============================] - 2s 1ms/step\n",
            "[[56838    26]\n",
            " [   21    77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.75      0.79      0.77        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.87      0.89      0.88     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldh5ZIQvndxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the credit card transaction dataset\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Balance the training dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the supervised autoencoder architecture\n",
        "input_dim = X_train_resampled.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the supervised autoencoder\n",
        "model.fit(X_train_resampled, y_train_resampled,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxagU3GJkmt_",
        "outputId": "74779ac6-13f6-4c1e-8643-3a6971dfe45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0303 - val_loss: 0.0072\n",
            "Epoch 2/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0057 - val_loss: 0.0076\n",
            "Epoch 3/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0039 - val_loss: 0.0061\n",
            "Epoch 4/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0030 - val_loss: 0.0073\n",
            "Epoch 5/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0026 - val_loss: 0.0067\n",
            "Epoch 6/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0022 - val_loss: 0.0071\n",
            "Epoch 7/10\n",
            "3554/3554 [==============================] - 11s 3ms/step - loss: 0.0020 - val_loss: 0.0064\n",
            "Epoch 8/10\n",
            "3554/3554 [==============================] - 11s 3ms/step - loss: 0.0019 - val_loss: 0.0063\n",
            "Epoch 9/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 10/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0013 - val_loss: 0.0077\n",
            "1781/1781 [==============================] - 3s 2ms/step\n",
            "[[56835    29]\n",
            " [   18    80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.73      0.82      0.77        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.87      0.91      0.89     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the credit card transaction dataset\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Balance the training dataset using SMOTE\n",
        "smote = SMOTE(random_state=20)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the supervised autoencoder architecture\n",
        "input_dim = X_train_resampled.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the supervised autoencoder\n",
        "model.fit(X_train_resampled, y_train_resampled,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6op9qVCUtaT7",
        "outputId": "e094c157-3f1e-49f7-c59a-ca4e9377ca58"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3554/3554 [==============================] - 10s 2ms/step - loss: 0.0287 - val_loss: 0.0095\n",
            "Epoch 2/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0056 - val_loss: 0.0082\n",
            "Epoch 3/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0039 - val_loss: 0.0088\n",
            "Epoch 4/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0029 - val_loss: 0.0074\n",
            "Epoch 5/10\n",
            "3554/3554 [==============================] - 11s 3ms/step - loss: 0.0027 - val_loss: 0.0071\n",
            "Epoch 6/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0020 - val_loss: 0.0074\n",
            "Epoch 7/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0021 - val_loss: 0.0070\n",
            "Epoch 8/10\n",
            "3554/3554 [==============================] - 9s 3ms/step - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 9/10\n",
            "3554/3554 [==============================] - 11s 3ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 10/10\n",
            "3554/3554 [==============================] - 10s 3ms/step - loss: 0.0015 - val_loss: 0.0075\n",
            "1781/1781 [==============================] - 3s 1ms/step\n",
            "[[56826    38]\n",
            " [   17    81]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.68      0.83      0.75        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.84      0.91      0.87     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, callbacks\n",
        "\n",
        "# Load the credit card transaction dataset\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Balance the training dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the supervised autoencoder architecture\n",
        "input_dim = X_train_resampled.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.2),  # Another Dropout layer\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron for binary classification\n",
        "])\n",
        "\n",
        "# Optimizer with a learning rate schedule\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "# Callbacks for early stopping and learning rate adjustment\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n",
        "\n",
        "# Train the supervised autoencoder\n",
        "model.fit(X_train_resampled, y_train_resampled,\n",
        "          epochs=30,  # Increased number of epochs\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[early_stopping, reduce_lr])  # Include callbacks in training\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu1cBUMongKB",
        "outputId": "1315fecc-4275-46b5-f93d-b5a4def4a699"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3554/3554 [==============================] - 17s 5ms/step - loss: 0.0553 - val_loss: 0.0174 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "3554/3554 [==============================] - 13s 4ms/step - loss: 0.0158 - val_loss: 0.0116 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0108 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0088 - val_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0074 - val_loss: 0.0090 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "3554/3554 [==============================] - 14s 4ms/step - loss: 0.0070 - val_loss: 0.0092 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0063 - val_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "3554/3554 [==============================] - 12s 3ms/step - loss: 0.0042 - val_loss: 0.0097 - lr: 1.0000e-04\n",
            "1781/1781 [==============================] - 3s 1ms/step\n",
            "[[56799    65]\n",
            " [   10    88]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.58      0.90      0.70        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.79      0.95      0.85     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    }
  ]
}